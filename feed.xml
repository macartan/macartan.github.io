<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://macartan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://macartan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-25T08:08:04+00:00</updated><id>https://macartan.github.io/feed.xml</id><title type="html">blank</title><subtitle>Macartan Humphreys </subtitle><entry><title type="html">Continuous treatments</title><link href="https://macartan.github.io/posts/ols/" rel="alternate" type="text/html" title="Continuous treatments"/><published>2024-12-18T00:00:00+00:00</published><updated>2024-12-18T00:00:00+00:00</updated><id>https://macartan.github.io/posts/ols</id><content type="html" xml:base="https://macartan.github.io/posts/ols/"><![CDATA[]]></content><author><name></name></author><category term="professional"/><category term="discussion"/><summary type="html"><![CDATA[Regressions of Y on X don't target the ATE]]></summary></entry><entry><title type="html">I saw your RCT and I have some questions</title><link href="https://macartan.github.io/posts/rct-faqs/" rel="alternate" type="text/html" title="I saw your RCT and I have some questions"/><published>2024-01-01T00:00:00+00:00</published><updated>2024-01-01T00:00:00+00:00</updated><id>https://macartan.github.io/posts/rct-faqs</id><content type="html" xml:base="https://macartan.github.io/posts/rct-faqs/"><![CDATA[ <ul id="markdown-toc"> <li><a href="#estimation" id="markdown-toc-estimation">Estimation</a> <ul> <li><a href="#nocontrols" id="markdown-toc-nocontrols">Controls: You didn’t control for &lt;this thing&gt;, should I be worried?</a></li> <li><a href="#should-i-be-worried-about-omitted-variable-bias" id="markdown-toc-should-i-be-worried-about-omitted-variable-bias">Should I be worried about ‘omitted variable bias’?</a></li> <li><a href="#controls-you-controlled-for-this-thing-should-i-be-worried" id="markdown-toc-controls-you-controlled-for-this-thing-should-i-be-worried">Controls: You controlled for &lt;this thing&gt;, should I be worried?</a></li> <li><a href="#i-saw-you-did-not-take-any-baseline-measures-should-i-be-worried" id="markdown-toc-i-saw-you-did-not-take-any-baseline-measures-should-i-be-worried">I saw you did not take any baseline measures!! Should I be worried?</a></li> <li><a href="#i-see-your-study-population-is-very-heterogeneous-people-differ-in-more-ways-than-i-can-think-of-should-i-be-worried" id="markdown-toc-i-see-your-study-population-is-very-heterogeneous-people-differ-in-more-ways-than-i-can-think-of-should-i-be-worried">I see your study population is very heterogeneous. People differ in more ways than I can think of. Should I be worried?</a></li> <li><a href="#imbalance" id="markdown-toc-imbalance">I am looking at your balance table and see that there are differences between the treatment and control group, should I be worried?</a></li> <li><a href="#i-can-see-that-people-in-the-control-group-could-have-been-affected-by-the-treatment-should-i-be-worried" id="markdown-toc-i-can-see-that-people-in-the-control-group-could-have-been-affected-by-the-treatment-should-i-be-worried">I can see that people in the control group could have been affected by the treatment, should I be worried?</a></li> <li><a href="#compliance" id="markdown-toc-compliance">I see you didn’t <em>really</em> control the treatment: in fact people could choose whether to take up the treatment or not! Should I be worried?</a></li> <li><a href="#attrition" id="markdown-toc-attrition">I see you didn’t <em>really</em> control the measurement: in fact people could choose whether to get measured or not. Should I be worried?</a></li> <li><a href="#the-outcome-is-binary-but-you-are-using-a-linear-estimator-should-i-be-worried" id="markdown-toc-the-outcome-is-binary-but-you-are-using-a-linear-estimator-should-i-be-worried">The outcome is binary but you are using a linear estimator! Should I be worried?</a></li> <li><a href="#i-see-you-only-have-100-observations-but-i-thought-the-arguments-supporting-inferences-from-rcts-required-really-large-samples-should-i-be-worried" id="markdown-toc-i-see-you-only-have-100-observations-but-i-thought-the-arguments-supporting-inferences-from-rcts-required-really-large-samples-should-i-be-worried">I see you only have 100 observations but I thought the arguments supporting inferences from RCTs required really large samples. Should I be worried?</a></li> </ul> </li> <li><a href="#inference" id="markdown-toc-inference">Inference</a> <ul> <li><a href="#pvalues" id="markdown-toc-pvalues">You wrote \(p =.03\). What does that mean?</a></li> <li><a href="#low_p" id="markdown-toc-low_p">If the <em>p</em> value is low does that mean that the estimated effect is reliable?</a></li> <li><a href="#ci" id="markdown-toc-ci">What is the 95% confidence interval?</a></li> <li><a href="#allequal" id="markdown-toc-allequal">Is every estimate inside a confidence interval equally likely?</a></li> <li><a href="#interpret" id="markdown-toc-interpret">Can I interpret the confidence interval as the range of values in which the true value probably lies?</a></li> <li><a href="#edge" id="markdown-toc-edge">I see the null is right at the edge of the confidence interval. Should I be worried?</a></li> <li><a href="#heterogeneous" id="markdown-toc-heterogeneous">I see that there is a significant effect for treatment A but not for treatment B, does that mean that treatment A is more effective than treatment B?</a></li> <li><a href="#heterogeneous2" id="markdown-toc-heterogeneous2">I see that there is a significant effect for group A (for instance, men) but not for group B (women), does that mean that the treatment is more effective for men than for women?</a></li> <li><a href="#underpowered" id="markdown-toc-underpowered">Someone said your study was “underpowered.” Should I be worried? (And what does that mean?)</a></li> <li><a href="#nullnull" id="markdown-toc-nullnull">If there is no significant treatment effect, does that mean that there is no effect?</a></li> <li><a href="#nullno" id="markdown-toc-nullno">If there is no significant effect does that mean that the intervention should not be implemented?</a></li> </ul> </li> <li><a href="#stepping-back" id="markdown-toc-stepping-back">Stepping back</a> <ul> <li><a href="#rct_ethics" id="markdown-toc-rct_ethics">This experiment does not look ethical to me, should I be worried?</a></li> <li><a href="#how-can-you-justify-withholding-a-beneficial-treatment-from-a-control-group" id="markdown-toc-how-can-you-justify-withholding-a-beneficial-treatment-from-a-control-group">How can you justify withholding a beneficial treatment from a control group?</a></li> <li><a href="#did-you-really-need-an-rct-to-show-this-i-am-worried-this-is-a-bad-use-of-resources" id="markdown-toc-did-you-really-need-an-rct-to-show-this-i-am-worried-this-is-a-bad-use-of-resources">Did you really need an RCT to show this? I am worried this is a bad use of resources.</a></li> </ul> </li> <li><a href="#notes" id="markdown-toc-notes">Notes</a></li> </ul> <h1 id="estimation">Estimation</h1> <p>An estimate is a guess about the value of some quantity of interest (the “estimand”). Here are questions people have when wondering if an estimate is valid.</p> <h2 id="nocontrols">Controls: You didn’t control for &lt;this thing&gt;, should I be worried?</h2> <p><strong>Probably not.</strong> Generally in an RCT you don’t <em>have</em> to control for anything in the following sense: in general, <em>your results are unbiased when you don’t control for anything</em>.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> This is true whether or not treatment and control groups <em>look</em> similar. Intuitively the reason is that any third thing, gender say, is <em>ex ante</em> no more likely to be overrepresented (or underrepresented) in the treatment group than in the control group, thanks to randomization. This is what trips people up: Unbiasedness is a statement about whether your estimates would be too high or too low <em>on average</em> if you repeated the experiment, not about whether you will be too high or too low in this instance.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> With that said, including background variables in estimation can help make sure that you make smaller errors (again, on average) and so you might want to see those variables included.</p> <p>So: you generally don’t need to be worried about things that are not controlled for if you are worried about bias. You might be worried if you think that unnecessarily imprecise estimation is making it too likely that the null hypothesis will be maintained when it should be rejected. Or if you worry that your unbiased estimate might well be very far from the truth (see for example some of the arguments in favor of blocking in <a href="http://www.ryantmoore.org/files/papers/blockPA.pdf">Multivariate Continuous Blocking to Improve Political Science Experiments</a>).</p> <h2 id="should-i-be-worried-about-omitted-variable-bias">Should I be worried about ‘omitted variable bias’?</h2> <p><strong>Probably not</strong>.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> See <a href="#nocontrols">here</a>. Even if you have imbalance on one or more background covariates (see <a href="#imbalance">here</a>).</p> <h2 id="controls-you-controlled-for-this-thing-should-i-be-worried">Controls: You controlled for &lt;this thing&gt;, should I be worried?</h2> <p><strong>Maybe</strong>. We sometimes control for background variables when looking at the difference between treatment and control outcomes. This is often done in order to produce more precise estimates. It’s often a good thing to do but there can be two things to worry about:</p> <ol> <li> <p>If a control is “post treatment” then including it can generate bias. You do not want to control for something that itself was affected by treatment. Say a medicine prevented death by reducing a fever. If you control for fever you might find that, conditional on fever, things look the same in the treatment and control groups and falsely conclude that the medicine had no effect. But of course its effect worked through the thing you just controlled for.</p> </li> <li> <p>While simple differences in means is guaranteed to be unbiased in an experiment, regression with adjustment is only guaranteed to be “consistent” (informally: works well when you have lots of data), and so you might worry when you have small samples. (See <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-7/issue-1/Agnostic-notes-on-regression-adjustments-to-experimental-data--Reexamining/10.1214/12-AOAS583.full">Lin</a> on this point and ways to address it.)</p> </li> <li> <p>Including controls can change the precision of estimates and sometimes test of hypotheses about estimates are “significant” when controls are used but not when they are not. In this case you <em>might</em> be worried that researchers have selected the controls <em>because</em> they made the result significant. This is a form of “<a href="https://www.cambridge.org/core/journals/political-analysis/article/fishing-commitment-and-communication-a-proposal-for-comprehensive-nonbinding-research-registration/BD935F7843BF07F338774DAB66E74E3C">fishing</a>” and can lead to false inferences. If researchers have pre-specified their controls in a pre-analysis plan then this is probably not a cause for concern.</p> </li> </ol> <p>For more on when to include covariates in your analysis (or when not to), see <a href="https://egap.org/resource/10-things-to-know-about-covariate-adjustment/">10 Things to Know About Covariate Adjustment</a>.</p> <h2 id="i-saw-you-did-not-take-any-baseline-measures-should-i-be-worried">I saw you did not take any baseline measures!! Should I be worried?</h2> <p><strong>No.</strong> Baseline measures are often a good idea and they can let you implement a better random assignment and get tighter estimates. <em>But</em> they are not needed for inference from an RCT. The fact that there are no baseline measures should not lead you to read estimates of effects differently or interpret standard errors or confidence intervals differently, since lower precision will normally be captured by these numbers already.</p> <p>There is confusion here because we intuitively think the goal is to estimate the difference between <em>changes</em> in the treatment and <em>changes</em> in the control group. But an amazing thing is that with randomization the differences in the average changes is <em>the same</em> as the difference in the average outcomes in treatment and control groups (in expectation).<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> So we are not talking about different quantities. A better way to think about baseline data is that it provides excellent controls, which you may or may not make use of (see <a href="#controls">here</a> and <a href="#nocontrols">here</a>).</p> <h2 id="i-see-your-study-population-is-very-heterogeneous-people-differ-in-more-ways-than-i-can-think-of-should-i-be-worried">I see your study population is very heterogeneous. People differ in more ways than I can think of. Should I be worried?</h2> <p><strong>Probably not.</strong> Inference based on randomization does not depend on homogeneity (a lovely reading on this is by <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1986.10478354?casa_token=Aqqys61_2McAAAAA:-ZoP77E53whDGKsJlKIkdU3QeOTDZF1cti0FpEn9DBheFU6jcKUvbAjw4g5SnLg4aSH36J4IW-00HUA">Paul Holland</a>). In particular there is no assumption that treatment works the same way for all people. Rather all we need (for unbiasedness) is that the average outcome in a random sample in any condition is the same (in expectation) as what the average outcome would be for the whole population in that condition.</p> <h2 id="imbalance">I am looking at your balance table and see that there are differences between the treatment and control group, should I be worried?</h2> <p><strong>Maybe.</strong> If you randomize then treatment and control groups should look fairly similar on pretreatment variables — gender, age, and so on. But of course in practice you can be certain they will look different.</p> <ul> <li>If they look <em>very very</em> different then you should wonder whether the randomization was in fact implemented correctly and investigate. See <a href="https://statmodeling.stat.columbia.edu/2021/08/22/does-the-table-1-fallacy-apply-if-it-is-table-s1-instead/">this discussion</a>.</li> <li>In cases in which you know treatment was really randomized but differences are not too large (large in substantive terms) you should probably not worry. Randomization is never going to generate perfect balance and in any case the “unbiasedness” that randomization brings does not depend on balance in specific instances. Moreover rest assured that the statistics we use, such as <em>p</em> values, are formed in the knowledge that there is not balance.</li> <li>If you are looking at a long list of <em>p</em> values, you may also be in a situation where a perfectly randomized experiment produces, say, 1/20 <em>p</em>-values less than .05 just by chance. In that case, you can (1) use an omnibus test for balance <a href="https://projecteuclid.org/journals/statistical-science/volume-23/issue-2/Covariate-Balance-in-Simple-Stratified-and-Clustered-Comparative-Studies/10.1214/08-STS254.full">based on randomization inference</a> as implemented in the <code class="language-plaintext highlighter-rouge">xBalance</code> command of the <a href="https://github.com/markmfredrickson/RItools">RItools</a> R pacakge or (2) adjust the <em>p</em>-values so that you are not over-interpreting some few small <em>p</em>-values that arise by chance (say, using the <code class="language-plaintext highlighter-rouge">p.adjust()</code> command in <code class="language-plaintext highlighter-rouge">R</code>).</li> <li>In cases in which you know treatment was really randomized, but you see large differences people sometimes say that randomization “failed.” That’s a bit misleading for the reasons given above (randomization delivers unbiasedness without guaranteeing balance). Even still, you might worry for the simple reason that you think the difference in outcomes is due to the imbalance not due to the treatment—especially if the imbalanced variable is likely to be strongly related to outcomes. There are strategies to address this but also dangers. See <a href="https://www.degruyter.com/document/doi/10.1515/jci-2015-0018/html">here</a> for a correction strategy and <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1322143">here</a> to get a sense of the dangers. You would probably want to see a discussion of strategies used in the paper in this case.</li> </ul> <h2 id="i-can-see-that-people-in-the-control-group-could-have-been-affected-by-the-treatment-should-i-be-worried">I can see that people in the control group could have been affected by the treatment, should I be worried?</h2> <p><strong>Probably.</strong> The standard analysis assumes that each subject reacts only to their own treatment status. If people react to other people’s status then simple analyses can produce the wrong results. For intuition if everyone gets healthy because half the population got a treatment then you will estimate an effect of 0 when in truth there was a positive effect for everyone.</p> <p>This is often called a problem of “spillovers” or, more technically, a violation of the “SUTVA” assumption. There are ways to address it and you should look to see whether people took account of this risk when doing analysis. <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-11/issue-4/Estimating-average-causal-effects-under-general-interference-with-application-to/10.1214/16-AOAS1005.full">Here</a> is a nice treatment of these issues. And see <a href="https://egap.org/resource/10-things-to-know-about-spillovers/">10 Things to Know about Spillovers</a> for a less technical overview.</p> <h2 id="compliance">I see you didn’t <em>really</em> control the treatment: in fact people could choose whether to take up the treatment or not! Should I be worried?</h2> <p><strong>Maybe not.</strong> It’s not uncommon that you can randomly “assign” a treatment but whether people actually take up the treatment is another matter. It could well be that only the people for whom the treatment works will actually take it up and others won’t bother. This problem goes by the name of “noncompliance.” Surprisingly this is less of a problem than it might seem. There are a couple of ways that researchers likely respond to this:</p> <ol> <li> <p>The “intention to treat” analysis is perhaps the most common. Researchers report on the effects of <em>assigning</em> a treatment, not the effects of taking up a treatment. This is often a policy relevant quantity. The fact that not everyone takes it up is not a problem here (rather it can be thought of as one of the things that dampen the effects of assigning people to treatment).</p> </li> <li> <p>Another approach is to focus on the “complier average treatment effect” or the “local average treatment effect”: the effect specifically for those that would take up the treatment if and only if they were assigned to treatment. For intuition you can estimate the effect of assignment on uptake and the effect of assignment on outcomes and then divide the latter by the former. For instance if giving a mask increases the probability of wearing a mask by 50 percentage points, and giving a mask reduced sickness by 10 percentage points. Then the estimated effect of wearing the mask (among those that wore a mask because you gave them a mask) is 20 percentage points; you can now think of the estimated 10 points as an average of 20 (for the compliers) and 0 (for the non compliers). The approach requires a few assumptions but it is often viable (the seminal reference is <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476902">Angrist et al</a>; see also <a href="https://declaredesign.org/blog/2019-02-05-instrumental-variables.html">here</a> for discussion of some assumptions and references). And see the discussion of the ITT in <a href="https://egap.github.io/learningdays-resources/Slides/estimation-slides.pdf">Estimating Estimands with Estimators</a> and <a href="https://egap.org/resource/10-things-to-know-about-the-local-average-treatment-effect/">10 Things to Know About the Local Average Treatment Effect</a>.</p> </li> <li> <p>Another approach is to focus only on people that took up the condition that they were assigned to and ignore the data of “non compliers” (this is sometimes called “per-protocol”). You should worry if you see this because it can produce biased estimates.</p> </li> </ol> <h2 id="attrition">I see you didn’t <em>really</em> control the measurement: in fact people could choose whether to get measured or not. Should I be worried?</h2> <p><strong>Maybe.</strong> Sometimes you end up getting measures for some people but not for other people and then people for whom you do not have measures end up dropping out of your sample. This can carry a risk of bias.</p> <p>For intuition imagine a drug has positive effects for for half your treatment group and negative effects for the other half. The ones that experienced negative effects are reasonably upset and stop taking part in the study. So you only see good outcomes in the treatment group. This is called “attrition.” If there is a lot of attrition you should check whether the authors have tried to address it. One approach is to try to demonstrate that the attrition was as good as random. Another is to show that attrition is unlikely large enough to change substantive conclusions. Possible responses are provided in Lin et al.’s <a href="https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.html#attrition">standard operating procedures</a>.</p> <h2 id="the-outcome-is-binary-but-you-are-using-a-linear-estimator-should-i-be-worried">The outcome is binary but you are using a linear estimator! Should I be worried?</h2> <p><strong>Probably not.</strong> Linear models generally work well for experimental data. People argue on this point but the differences in estimates of the ATE from different approaches is often not large.</p> <p>In simple set ups with experimental data, linear (OLS) models return the difference in means estimate, which is unbiased, no matter the data type: and with a binary outcome the difference of means is a difference in proportions.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> With that said there are lots of different approaches that can be used to “model” different types of data generating processes, such as for binary data, ordinal data, or count data. These can get at quantities that linear models do not aim for and that can be a reason to use them. But warning: they might not be <a href="https://projecteuclid.org/journals/statistical-science/volume-23/issue-2/Randomization-Does-Not-Justify-Logistic-Regression/10.1214/08-STS262.full">justified</a> by randomization and might not be <a href="https://declaredesign.org/blog/estimating-average-treatment-effects-with-ordered-probit-is-it-worth-it.html">worth it</a> if you are interested in the average treatment effect.<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup></p> <h2 id="i-see-you-only-have-100-observations-but-i-thought-the-arguments-supporting-inferences-from-rcts-required-really-large-samples-should-i-be-worried">I see you only have 100 observations but I thought the arguments supporting inferences from RCTs required really large samples. Should I be worried?</h2> <p><strong>Probably not.</strong></p> <ul> <li>Sometimes people say that you need a large sample to be sure that estimates are not biased. But larger samples are about precision not bias. You can get unbiased estimates with only 1 unit in treatment and 1 in control.<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup></li> <li>People often invoke normal distributions and the like when describing procedures for calculating <em>p</em> values. But you can calculate <em>p</em> values and confidence intervals for “sharp” hypotheses<sup id="fnref:8"><a href="#fn:8" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> with very few units without making any assumptions about distributions, just using what you know about the <a href="https://egap.org/resource/10-things-to-know-about-randomization-inference/">randomization</a>.</li> <li>Sometimes you can run into problems calculating standard errors when there are too few clusters. Good choices of method for calculating standard errors can help a lot though even with small samples. See <a href="https://academic.oup.com/ectj/article/21/2/114/5078969?login=true">here</a> and <a href="https://declaredesign.org/blog/how-misleading-are-clustered-ses-in-designs-with-few-clusters.html">here</a>.</li> <li>Deaton &amp; Cartwright raise the concern that the procedure for calculating <em>p</em> values (from normal or t distributions) is not justified by randomization. See a response to this <a href="https://declaredesign.org/blog/randomization-does-not-justify-t-tests.-how-worried-should-i-be.html">here</a> that suggests this is not likely to be major concern.</li> <li>If you want to you can <em>find out</em> how far you are from normality under the sharp null of no effects, given the data at hand. See illustration and code <a href="https://macartan.github.io/i/notes/CLT_RCT.html">here</a>.</li> </ul> <hr/> <h1 id="inference">Inference</h1> <h2 id="pvalues">You wrote \(p =.03\). What does that mean?</h2> <p>That is my guess at the probability we would see such a difference between treatment and control groups if in fact the treatment had no effect. If the <em>p</em> value is low then I recommend that you stop thinking that there might not have been an effect.</p> <h2 id="low_p">If the <em>p</em> value is low does that mean that the estimated effect is reliable?</h2> <p>Informally that is the conclusion we tend to draw but technically a very low <em>p</em> value is not a statement about the effect you estimated but about the hypothesis of no effect.</p> <p>Frequentist statistics are not designed to learn about the probability that one or other effect is correct but rather whether you should doubt one effect or another. For claims about the probability of a given effect see Bayesian inference.</p> <h2 id="ci">What is the 95% confidence interval?</h2> <p>The confidence interval is a surprisingly obscure object.</p> <p><strong>The wrong interpretation</strong>: Informally people think of it as a describing a set of possible values such that there is a 95% probability that the truth lies within the confidence interval. This is wrong because the construction of the confidence interval does not use any information about how likely one or other possible effect is. It uses information about how likely the data are <em>given</em> some effect or other.</p> <p><strong>The technical feature that the confidence interval should have</strong>: The confidence interval is a pair of functions producing upper and lower bounds (an interval) designed so that if we repeated the experiment, and recalculated the interval, there would be a 95% chance (or more) that the interval would include the true effect (it would “cover” the truth 95% of the time).<sup id="fnref:9"><a href="#fn:9" class="footnote" rel="footnote" role="doc-noteref">9</a></sup> Now that’s a pretty hard idea to make sense of. And although it sounds a lot like it, what you <em>cannot</em> get from this definition is that the particular (confidence) interval we <em>reported</em> includes the true effect with 95% probability.<sup id="fnref:10"><a href="#fn:10" class="footnote" rel="footnote" role="doc-noteref">10</a></sup> It either does include it or it does not. This definition doesn’t help much in knowing what to do with the particular interval you have before you. (Though see <a href="https://statmodeling.stat.columbia.edu/2013/01/14/how-do-you-think-about-the-values-in-a-confidence-interval/">here</a> for a lead)</p> <p><strong>An interpretation we can understand.</strong> The interval can often be interpreted as <em>the set of values that you cannot reject (at the 5% level) given the data</em>.<sup id="fnref:11"><a href="#fn:11" class="footnote" rel="footnote" role="doc-noteref">11</a></sup> Values outside the set are values that you <em>can</em> reject (at the 5% level). So one (“frequentist”) approach is to reject all values outside the confidence interval and maintain all those inside. This interpretation is behind the common practice of saying “0 is inside my confidence interval so I cannot distinguish my estimated effect from 0.”</p> <h2 id="allequal">Is every estimate inside a confidence interval equally likely?</h2> <p><strong>No</strong> (or maybe: no basis for the conclusion).</p> <p>There is a sense that every number inside a confidence interval should be treated the same way: they are all values that you do not reject.</p> <p>But that does not mean that they are all equally <em>likely</em>. For a statement about whether an estimate (or range of estimates) is <em>likely</em> you need some prior about whether they are likely. In other words you need information that the confidence interval doesn’t use and doesn’t provide. But see <a href="#interpret">here</a>.</p> <p>Also, the <em>p</em>-values for values near the edges of the 95% CI will be near .05 and near the center of that interval the (two-sided) <em>p</em>-values will be higher than .05.</p> <h2 id="interpret">Can I interpret the confidence interval as the range of values in which the true value probably lies?</h2> <p><strong>At a stretch.</strong> In practice people think of confidence intervals as if they were 95% Bayesian “<a href="https://en.wikipedia.org/wiki/Credible_interval">credibility intervals</a>” (a range that holds the true value with 95% probability) and think of numbers in the middle as more likely.</p> <p>Technically this is incorrect. The confidence interval does not use or provide information about the probability of one effect or another.</p> <p>But even though the informal practice is not technically correct, it is not completely crazy either. In some applications the confidence interval can look a lot like the credibility interval. For intuition: Imagine I thought that if the true effect were \(\tau\) the estimates I would get from my experiment would be distributed normally across experiments, \(t \sim N(\tau, 0.5)\). Say I estimate \(t = 1\). Then my confidence interval would be something like \([0.02, 1.98]\). If I further supposed that ex ante I thought any value of \(\tau\) equally likely, then my posterior belief would be proportional to the likelihood of observing 1 under each possible value of \(\tau\). The most likely value would indeed be \(\tau = 1\) and I would put about 95% probability mass on the possibilities in the credibility interval. I would put <em>much</em> lower weight on values near the edge of the confidence interval. (Bayesian plug: If you want to give a Bayesian interpretation ask for a Bayesian analysis from the get go).</p> <h2 id="edge">I see the null is right at the edge of the confidence interval. Should I be worried?</h2> <p><strong>Maybe</strong>. There is a sense in which if 0 is right at the edge of a confidence interval that your estimate was <em>almost</em> not significant. Or put differently, perhaps you can reject the null of 0 but you cannot reject the null of an absolutely tiny effect. This is the key thing: there is nothing truly sharp about these thresholds, there is no real difference being just inside or just outside a confidence interval in the same way as there is no real difference between \(p = .051\) and \(p = .049\). In particular, <em>tiny differences have almost no bearing on what effects are “likely”</em>. As <a href="http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf">Gelman and Stern</a> nicely put it: the difference between “significant” and “not significant” is not itself statistically significant. See also <a href="#allequal">this</a>.</p> <h2 id="heterogeneous">I see that there is a significant effect for treatment A but not for treatment B, does that mean that treatment A is more effective than treatment B?</h2> <p><strong>No.</strong> Weirdly it is possible that you can distinguish A’s effect from 0 and not distinguish B’s effect from 0, but still not be able to rule out that A and B have the same effect. It’s even possible that you have a significant effect for A and not B but that your estimated effect is bigger for B than for A.</p> <p>For intuition, imagine you had a tight estimate for treatment A, with a mean of 1 and a confidence interval of [.9, 1.1]. For B your estimate is very noisy, with a mean of 2 but confidence interval of [-1, 5]. Here your estimate for B is so noisy that you cannot distinguish it from 0 or from 1. You would be wrong to infer that A is more effective than B.</p> <p>If you are reading a paper and statements are made about the relative effects of treatments, look for the analyses that specifically back up these claims.</p> <h2 id="heterogeneous2">I see that there is a significant effect for group A (for instance, men) but not for group B (women), does that mean that the treatment is more effective for men than for women?</h2> <p><strong>No.</strong> See <a href="heterogeneous2">here</a>.</p> <p>If you are reading a paper and statements are made about the different effects of treatments for different groups, look for the analyses that specifically back up these claims. These often come in the form of an interaction term: Treatment \(\times\) Gender.</p> <h2 id="underpowered">Someone said your study was “underpowered.” Should I be worried? (And what does that mean?)</h2> <p><strong>Maybe.</strong> Your study is underpowered if (ex ante) you are not very likely to reject a null of no effect even when there is a real effect. The basic problem is that the design is not capable of generating sufficiently precise estimates to be able to distinguish real effects from noise. The problem with that is that you might (falsely) conclude that there is no effect even though there is one. Having more observations generally improves power and underpowered studies are often small. But other things matter also, such as how randomization was done, how measurement was done, and what <a href="https://declaredesign.org/blog/improve-power-using-your-answer-strategy-not-just-your-data-strategy.html">estimators</a> are used.</p> <p>If you are worried about power it can be useful to look at confidence intervals. An underpowered study has big confidence intervals. Before discounting an intervention on the grounds that you cannot reject the null of no effect you might also ask whether you can reject the possibility that there are very large effects. If you cannot reject either large or small effects you might conclude that there is not too much to learn here.</p> <p>You might think that low power is not a problem if in fact you get a significant result. But there’s a subtlety here. <em>Conditional</em> on being significant, an underpowered estimate is more likely than not to be an overestimate. If someone brings your attention to an underpowered published study with significant results and huge effects it’s probably too good to be true. (Gelman calls this the <a href="https://statmodeling.stat.columbia.edu/2011/09/10/the-statistical-significance-filter/">significance filter</a>; we discuss implications for publishing <a href="https://declaredesign.org/blog/a-journal-of-null-results-is-a-flawed-fix-for-a-significance-filter.html">here</a>)</p> <p>For more on power, see <a href="https://egap.org/resource/10-things-to-know-about-statistical-power/">10 Things to Know About Statistical Power</a>.</p> <h2 id="nullnull">If there is no significant treatment effect, does that mean that there is no effect?</h2> <p><strong>No.</strong> People sometime say: “absence of evidence is not evidence of absence” which captures the idea that in general, null results should be interpreted as absence of evidence. A null result just means that data like this could have been produced easily enough by a process in which there is no true effect.<sup id="fnref:12"><a href="#fn:12" class="footnote" rel="footnote" role="doc-noteref">12</a></sup> The problem is not helped by the fact that unfortunately researchers often say “we found that this treatment didn’t work” when really they mean “we weren’t able to rule out the possibility that there is no effect.”</p> <p>Better in this case to look at the confidence interval to see whether they are tight. If you have a null result with tight confidence intervals that means that large effects are rejected—bigger effects sizes, outside the confidence interval, are not likely to have produced data like this.</p> <p>For more on interpreting null results, see <a href="https://egap.org/resource/10-things-your-null-result-might-mean/">10 Things Your Null Result Might Mean</a>.</p> <h2 id="nullno">If there is no significant effect does that mean that the intervention should not be implemented?</h2> <p><strong>No.</strong> A lot of the statistics commonly used for analyzing RCTs are for figuring out if the data support some or other hypothesis. But the decision to take an action should depend on what you believe the costs and benefits are and knowing that an estimate is “significant” does not help much with that decision.</p> <p>As a concrete example consider a version of the problem described <a href="#intepret">here</a> and imagine that if the true effect were \(\tau_j\) the estimates I would get from my experiment would be distributed normally if I were to repeat my experiment many times, \(t \sim N(\tau_j, 1)\). Say I estimate \(t = 1\). Then my confidence interval would be something like [-1, 3]. So I would have a null result, with an estimate that is quite far from significant. Say though that my benefit from intervening was \(\tau\) but my cost was .5. Then my <em>expected</em> net benefit from intervening would be 1-.5 = .5. If my benefit were \(\tau^2\) then my expected net benefit would be 2 - .5 = 1.5. So I have reason to take an action even though I cannot reject the null of no effect.</p> <p>The bigger point is that when using data to make inferences you should not focus on significance alone, or on the estimate of effect sizes alone, but try to think through the implications given your beliefs over a range of plausible effects.</p> <h1 id="stepping-back">Stepping back</h1> <h2 id="rct_ethics">This experiment does not look ethical to me, should I be worried?</h2> <p><strong>Maybe</strong>. There is increasing attention to the importance of attending to ethics in experiments (and other research). RCTs invoke specific ethical concerns insofar as they, typically, manipulate some people in order for other people to learn things. With that said they are often implemented specifically to learn things that could be beneficial to participants and to others and in ways that are equitable and avoid harm to participants. There is however variation in the extent to which they involve proper informed consent—informed consent is common for measurement but less common for social science interventions.</p> <p>If you have concerns about the ethics reach out to the authors. There may be features of the intervention that address your questions that are not in the paper. It’s useful to draw authors’ attention to these issues because such features <em>should</em> be in the paper.</p> <ul> <li><a href="https://www.poverty-action.org/blog/call-structured-ethics-appendices-social-science-papers">Karlan and Udry</a> have proposed including “Structured Ethics Appendices” in papers.</li> <li><a href="https://egap.org/wp-content/uploads/2020/05/egap-research-principles.pdf">EGAP</a> has a set of research principles focused on collaborations in a particular</li> <li>For political scientists, <a href="https://connect.apsanet.org/hsr/principles-and-guidance/">APSA</a> has principles and guidance to help navigate some of the issues.</li> <li>For more on the specific ethical considerations those working in conflict areas should follow, see <a href="https://egap.org/resource/standards-discussion-ethics-violent-contexts/">Transparency in research ethics: Methods to monitor principles and practice in violent contexts</a>.</li> </ul> <h2 id="how-can-you-justify-withholding-a-beneficial-treatment-from-a-control-group">How can you justify withholding a beneficial treatment from a control group?</h2> <p>There have been cases where beneficial treatments have been withheld from control groups for research purposes which raises obvious ethical concerns. If you are worried about this, first find out if that is what actually happened. A number of the items in <a href="https://www.poverty-action.org/blog/call-structured-ethics-appendices-social-science-papers">Karlan and Udry’s</a> framework help assess this point but the information is still often missing in writeups.</p> <p>Sometimes what looks like straight up withholding benefits has some or more of the following features:</p> <ul> <li>Treatments were not withheld but differentially encouraged in the treatment group. For instance an RCT encourages a group to wear masks but does nothing to prevent control groups from wearing masks. (See <a href="#compliance">here on compliance</a>)</li> <li>The number of beneficiaries was set independent of the RCT and the RCT was the mechanism to determine allocation, perhaps by expanding the eligible pool.</li> <li>The RCT involved differences in timing for when benefits were delivered, not a change in who benefited. Sometimes this is done in settings where benefits are naturally rolled out over time anyhow.</li> <li>The benefits were not known before the study. In some cases if treatments are found to be beneficial they are then extended to control groups.</li> </ul> <h2 id="did-you-really-need-an-rct-to-show-this-i-am-worried-this-is-a-bad-use-of-resources">Did you really need an RCT to show this? I am worried this is a bad use of resources.</h2> <p>A main reason to use RCTs is that different types of “selection bias” can result in very misleading inferences when we look at patterns as they occur naturally in the world. For instance development programs might operate in easy-to-access areas. When we compare places with and without programs we might think that the program is very effective because many other good things are going on in easy-to-access areas and so we mistakenly think that the program is effective when it is not. Similarly we might find no differences between areas in which a program is in operation and those in which it is not and wrongly conclude that there is no effect—rather the program chose disadvantaged areas to work in and so the lack of a difference is actually evidence of an effect. The random assignment to treatment provides confidence that none of these selection biases are producing faulty inferences.</p> <p>If you are worried about whether an RCT was justified it can be helpful to (i) think through what are the types of biases in this case that might undermine inferences from observational comparisons (or: are there reasons to think that you might be wrong about what you think an intervention does) (ii) compare the costs of the RCT to the costs and benefits of the intervention—a million dollar RCT that gives guidance on a billion dollar intervention can be money well spent (iii) try to think back to what you thought before seeing the result of the RCT and benchmark learning against that, likely either your beliefs about effect sizes changed <em>or</em> your confidence in your beliefs, or both (iv) try to think through how you might have changed your beliefs had the RCT produced outcomes different to what it did.</p> <h1 id="notes">Notes</h1> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>There are some exceptions: for instance if you randomize with different probabilities within blocks then you should control for those blocks: a great approach is estimate the effects within each block and then combine the estimates across blocks. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>In practice you may have more men in treatment and more women in control, say. And it may well be that your estimate of effects partly reflects this, in the sense that you would have gotten different estimates if it were the other way around. But your estimators is <em>unbiased</em> for the simple reason that bias is a statement about whether you expect to be right on <em>average</em> and not a statement about whether you are right this time. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>The rare cases in which you might worry are if the design used a randomization procedure — such as assigning different units with different probabilities — that calls for adjustment at the analysis stage. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>For a bit more intuition the average of the differences \((\overline{y}^T_{t=1} - \overline{y}^T_{t=0}) - (\overline{y}^C_{t=1} - \overline{y}^C_{t=0})\) can also be written as \((\overline{y}^T_{t=1} - \overline{y}^C_{t=1}) - (\overline{y}^T_{t=0} - \overline{y}^C_{t=0})\), where the first part is just the difference between endline outcomes and the second part is zero in expectation, thanks to randomization. A slightly deeper (and simpler) way to think about it is in terms of the estimand: if we are interested in average changes we are interested, in the average, over individuals, of \((Y_i^{t=1}(1) - Y_i^{t=0}) - (Y_i^{t=1}(0) - Y_i^{t=0}) = Y_i^{t=1}(1) - (Y_i^{t=1}(0))\), where \(Y_i^{t=1}(x)\) is a potential outcome—the value \(Y\) takes in condition \(x\)—but \(Y_i^{t=0}\) is a realized outcome. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5"> <p>One word of caution: if the data is ordinal, say taking on values (0 = unhappy, 1 = happy, 2 = very happy) the ATE you estimate with OLS is the effect of the treatment on the scale you are using. If you decided that “very happy” should be coded as a 3 not a 2 you would get different answers. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6"> <p>For discussion of gains in the case with covariates see <a href="https://www.tandfonline.com/doi/full/10.1080/07474938.2020.1824732">Negi and Woolridge</a> (Part 7). <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7"> <p>In fact using a <a href="https://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator">Horvitz Thompson</a> estimator you can get unbiased estimates if you have only one observation in total! <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:8"> <p>e.g. the null that the treatment had no effect on all units. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:9"> <p>It is often conceived of as a type of set valued estimate, but in practice it is used to provide a sense of uncertainty about a point estimate or range of plausible hypotheses — hypotheses that, if tested, would not yield \(p &lt; .05\) in the case of a 95% CI. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:10"> <p>Back in 1954 Savage complained about how routinely this has to be pointed out. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:11"> <p>For intuition: Say your <em>p</em> values are calculated correctly and so if the truth is 0 you will declare it to have a <em>p</em> value below 0.05 just 5% of the time, then 95% of the time 0 will be within the set that you do not reject. Similarly for other null hypotheses that you might consider. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4877414/">Greenland et al</a> highlight many more fallacies and clarify conditions under which this interpretation is valid. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:12"> <p>At the same time, any Bayesian would likely argue that a null result suggests that there may be nothing going on. Expressing the idea properly requires a Bayesian set up however. See <a href="#interpret">here</a>. <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[A helper for practitioners and others reading analyses of RCTs.]]></summary></entry><entry><title type="html">Global vaccination letdown (in German)</title><link href="https://macartan.github.io/posts/vaccination/" rel="alternate" type="text/html" title="Global vaccination letdown (in German)"/><published>2022-02-01T00:00:00+00:00</published><updated>2022-02-01T00:00:00+00:00</updated><id>https://macartan.github.io/posts/vaccination</id><content type="html" xml:base="https://macartan.github.io/posts/vaccination/"><![CDATA[]]></content><author><name></name></author><category term="health"/><summary type="html"><![CDATA[On the failure to share vaccinations]]></summary></entry><entry><title type="html">Democracy and Climate Action</title><link href="https://macartan.github.io/posts/democracy-climate-action/" rel="alternate" type="text/html" title="Democracy and Climate Action"/><published>2022-01-07T00:00:00+00:00</published><updated>2022-01-07T00:00:00+00:00</updated><id>https://macartan.github.io/posts/democracy-climate-action</id><content type="html" xml:base="https://macartan.github.io/posts/democracy-climate-action/"><![CDATA[<p>I enjoyed following the discussion between Ross Mittiga and Alexander Wuttke last week on climate action and authoritarian policies (<a href="https://twitter.com/RossMittiga/status/1476996563272622080">threads</a>:<a href="https://www.cambridge.org/core/journals/american-political-science-review/article/abs/political-legitimacy-authoritarianism-and-climate-change/E7391723A7E02FA6D536AC168377D2DE">paper</a>). Both Wuttke and Mittiga engaged constructuively on an issue they care a lot about and the discussion clarified many tricky points. It’s exactly the kind of cross subfield discussion between theorists and empirical researchers that we need more of (though I am not convinced twitter is the place for it). Cross-field criticisms are sometimes not welcomed—e.g. we saw charge against amateur theorists not staying in lane—but Mittiga’s response was just a model of grace.</p> <p>The discussion raises, I think, some interesting questions for work that depends on both normative and empirical claims—as does a lot of theoretical work in political science. Below I summarize where I think things ended up and where I think there are points of confusion. I end with a question about whether or when theory papers should provide support for empirical premises on which a normative conclusion depends.</p> <h3 id="argument-summary">Argument summary</h3> <p>There are lots of interesting parts to Mittiga’s paper, but I think the core of the argument relates to three propositions. (Note this is just my attempt to reconstruct the argument; it is not found in this form in the text so apologies to Ross if I mix it up.)</p> <p>The propositions:</p> <h4 id="propositions">Propositions</h4> <ul> <li>A: Any policy that is incompatible with survival of a polity is ipso facto not legitimate</li> <li>B: Non-authoritarian policies are incompatible with the survival of a polity in situation X (a state of exception)</li> <li>C: Authoritarian policies are legitimate in situation X</li> </ul> <p>We want to get from <em>A</em> and <em>B</em> to <em>C</em>. In my reading we can’t quite get there directly since A and B only get us to “Non-authoritarian policies are not legitimate in situation <em>X</em>.” So we seem to need something more. One possibility might be to add an assumption:</p> <p><strong>Assumption O</strong>: In any situation some set of policies is always legitimate.</p> <p>Under <em>Assumption O</em>, <em>C</em> is implied by <em>A</em> and <em>B</em>.</p> <p>If we don’t maintain <em>Assumption O</em> then we could still get to <em>C</em> with variations on <em>A</em> and <em>B</em> (<em>A’</em> : <strong>“Any policy that is required for the survival of a polity is ipso facto legitimate“</strong>; <em>B’</em> : <strong>“Authoritarian policies are required for the survival of a polity”</strong> <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>) Other strategies might involve moving beyond necessity and sufficiency statements to more continuous statements (such as “better able to ensure the survival of a polity”).</p> <p>There are two further distinct types of conclusion that follow from <em>C</em>:</p> <h4 id="conclusions">Conclusions</h4> <ul> <li>D: “Avoiding legitimation of authoritarian policies requires avoiding situation X”</li> <li>E: “A and B imply D”</li> </ul> <p>We might call <em>D</em> the strong conclusion—it follows if <em>A</em> and <em>B</em> are true (assuming <em>O</em> or similar); and <em>E</em> the weak conclusion, an analytic claim of the if-then variety that does not require <em>A</em> and <em>B</em> to be true.</p> <h4 id="contributions">Contributions</h4> <p>I think the key contribution of the article is claim <em>E</em> not claim <em>D</em> (though I have some confusion on this, see below). That is, to point out that “if <em>A</em> and <em>B</em> are true then <em>C</em> and <em>D</em> are true.” Sure enough, given Assumption <em>O</em>, <em>C</em> follows from <em>A</em> and <em>B</em>. And <em>D</em> follows from <em>C</em>. <em>D</em> then is useful insofar as it connects the dots and highlights an important conclusion for anyone who thinks <em>A</em> and <em>B</em> are plausible.</p> <p>A second contribution, I think, is to argue that <em>A</em> is true.</p> <h3 id="points-of-confusion">Points of confusion</h3> <p>Beyond the missing assumption noted above, I see a few points of confusion (that is, where I was confused). Some of these have been addressed in the discussion but not all:</p> <p><strong>Regimes or policies?</strong> The term “authoritarian governance” is not too clear to me: at some points it sounds like authoritarian <em>regimes</em> are meant, not authoritarian policies implemented by democracies (the title mentions “authoritarianism”; the early discussion compares states of different regime types); Mittiga’s responses emphasize that policies not regimes are meant and this is also laid out quite clearly in the section “Authoritarian Climate Governance”.</p> <p><strong>Strong or weak claim?</strong> I think it is sometimes unclear whether Mittiga is making the stronger or weaker claim. At some points it sounds like Mittiga is in fact supposing <em>A</em> and <em>B</em> are indeed true and so inferring that <em>C</em> and <em>D</em> are indeed true. Thus claiming <em>D</em> itself, rather than the weaker claim “If <em>A</em> and <em>B</em> then <em>D</em>”. The last line of the abstract for instance seems to state <em>D</em> directly, but it is caveated in a way that suggests to me that the weaker claim is meant (because of the uses of “suggests” / “may” and, critically, an incorporation of a version of <em>A</em> as a condition of the statement itself). The conclusion uses somewhat similar wording that seems more in line with the weak claim.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> The tweeted responses suggest though that the strong (“disturbing”) claim is meant.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> <p><strong>Is <em>A</em> true by definition?</strong> There are parts of the text that suggests <em>A</em> is true by definition, but ultimately I think <em>A</em> is meant as a normative claim. When foundational legitimacy (<em>FL</em>) is defined we are told that “<em>FL</em> requires that citizens’ essential safety needs are met.” So meeting needs seems to be part of the definition of <em>FL</em>. Later we have “<em>legitimacy</em> requires that governments ensure the safety and security of their citizens” (emphasis added). I understand this to mean that <em>FL</em> is not definitional to legitimacy, but rather, following the argumentation in the text, that in fact <em>FL</em> is necessary (though not sufficient) for legitimacy. Thus <em>CL</em> and <em>FL</em> are not types of legitimacy, but rather are things that can contribute in some way to legitimacy (indeed, sometimes: “political legitimacy will depend more (or even exclusively) on whether security needs are met than on whether CL factors are satisfied”).</p> <p><strong>Is B true by definition?</strong> There’s a part of the text that suggests that <em>B</em> might be set up to be true <em>by definition</em> but ultimately I think <em>B</em> is meant as an empirical proposition. The challenges arise where there is a “state of exception,” where a “state of exception […] is precipitated by an emergency (or credible threat thereof) of sufficiently great magnitude that prevailing political institutions, processes, norms, etc. either impede the swift action needed to preserve/restore normal conditions or simply break down”. This sounds like the inability of democratic institutions to address the problem is part of the criteria determining whether you have an SOE. But such a tautology would I think defang claim D; and elsewhere versions of <em>B</em> seem to be treated as empirical claims not a tautology (indeed: “ultimately an empirical question whether authoritarian governance is better able to realize desired environmental outcomes”). So the key part of <em>B</em> , not contained in the definition, is the idea that authoritarian governance would fare better (or: some policies will work).</p> <p><strong>Is a still stronger claim implied?:</strong> some of the reactions were to a stronger claim than Mittiga made. But there might be a reason for this. If you in fact buy <em>A</em> and <em>B and</em> you believe we <em>now</em> are in situation <em>X</em> (understood as a crisis), then the same logic would lead you not to <em>D</em> but to: “<em>D</em>’: authoritarian governance is legitimate.” So a more extreme conclusion can follow depending on what you think about <em>X</em> — the state of degradation of the climate.</p> <h3 id="combining-theoretical-and-empirical-propositions">Combining theoretical and empirical propositions</h3> <p>This discussion has been interesting because it was between an empirical and a theoretical researcher. Evaluation of the argument calls, I think, for that kind of discussion. The conclusions depend on what you think of <em>A</em> (a normative claim) and <em>B</em> (an empirical claim). So attention naturally goes to these.</p> <p><strong>A</strong> (the normative claim): Assuming <em>A</em> is not meant to be true by definition I’d like to learn more about why we should believe A. Addressing this requires a sharper statement of legitimacy (and not just of FL and CL) than we have in the text, I think. It doesn’t seem hard to me to imagine notions of legitimacy that are consistent with self sacrifice: taking actions that threaten survival but are good for other reasons purpose. Does <em>A</em> hold broad support among theorists?</p> <p><strong>B</strong> (the empirical claim): Wuttke is concerned with <em>B</em> and I think both sides agree that you can’t use Mittiga’s argument to get to <em>D</em> if you don’t buy <em>B</em>. Mittiga doesn’t do a lot to establish <em>B</em> however. Should he? More generally, what do we think is the responsibility of theorists to establish the plausibility of a condition on which a normative conclusion (here <em>C</em> and <em>D</em>) depends? I think the answer depends in part on whether claims like the strong claim or the weak claim are meant. If the strong claim is meant then <em>B</em> really is needed for the argument and Wuttke’s concerns are critical. If the weak claim is meant then the <em>importance</em> of the claim (rather than the validity of the claim) depends on whether <em>B</em> is plausible. In that case might it be enough to argue that many people believe B or at least that many believe <em>B</em> to be possible?</p> <hr/> <p><strong>Notes</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I invoke Assumption O rather than the stronger version of A because Mittiga emphasizes that ensuring survival may be necessary but is not sufficient for legitimacy (with a nice discussion of Hobbes and Williams). A difficulty with Assumption O is that it is inconsistent with A if no policies are consistent with survival. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>“But if adhering to CL factors proves incompatible with responding effectively to the climate crisis, then political legitimacy may require adopting a more authoritarian approach” (emphasis added). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>“In short, the article is an attempt to draw our attention to one of the most significant threats to our ability to maintain democratic institutions and robust human rights… Should those of us fortunate enough to live in liberal-democratic states fail to respond adequately to the climate crisis, we open the door to legitimating darker, more authoritarian forms of climate governance.” <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="discussion"/><summary type="html"><![CDATA[I enjoyed following the discussion between Ross Mittiga and Alexander Wuttke last week on climate action and authoritarian policies (threads:paper). Both Wuttke and Mittiga engaged constructuively on an issue they care a lot about and the discussion clarified many tricky points. It’s exactly the kind of cross subfield discussion between theorists and empirical researchers that we need more of (though I am not convinced twitter is the place for it). Cross-field criticisms are sometimes not welcomed—e.g. we saw charge against amateur theorists not staying in lane—but Mittiga’s response was just a model of grace.]]></summary></entry><entry><title type="html">Ambiguous implications of collider bias for the estimation of discrimination</title><link href="https://macartan.github.io/posts/bias-both-ways/" rel="alternate" type="text/html" title="Ambiguous implications of collider bias for the estimation of discrimination"/><published>2020-06-29T00:00:00+00:00</published><updated>2020-06-29T00:00:00+00:00</updated><id>https://macartan.github.io/posts/bias-both-ways</id><content type="html" xml:base="https://macartan.github.io/posts/bias-both-ways/"><![CDATA[]]></content><author><name></name></author><category term="professional"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Discrimination Estimands</title><link href="https://macartan.github.io/posts/collider-estimands/" rel="alternate" type="text/html" title="Discrimination Estimands"/><published>2020-06-29T00:00:00+00:00</published><updated>2020-06-29T00:00:00+00:00</updated><id>https://macartan.github.io/posts/collider-estimands</id><content type="html" xml:base="https://macartan.github.io/posts/collider-estimands/"><![CDATA[]]></content><author><name></name></author><category term="professional"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Conditioning on a collider (a bit technical)</title><link href="https://macartan.github.io/posts/collider-independence-technical/" rel="alternate" type="text/html" title="Conditioning on a collider (a bit technical)"/><published>2020-06-29T00:00:00+00:00</published><updated>2020-06-29T00:00:00+00:00</updated><id>https://macartan.github.io/posts/collider-independence-technical</id><content type="html" xml:base="https://macartan.github.io/posts/collider-independence-technical/"><![CDATA[]]></content><author><name></name></author><category term="professional"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">An instrument does not have to be exogenous to be consistent</title><link href="https://macartan.github.io/posts/DD-instrument/" rel="alternate" type="text/html" title="An instrument does not have to be exogenous to be consistent"/><published>2019-02-19T00:00:00+00:00</published><updated>2019-02-19T00:00:00+00:00</updated><id>https://macartan.github.io/posts/DD-instrument</id><content type="html" xml:base="https://macartan.github.io/posts/DD-instrument/"><![CDATA[]]></content><author><name>DeclareDesign</name></author><category term="methods"/><category term="discussion,"/><category term="DeclareDesign"/><summary type="html"><![CDATA[We often think of an instrumental variable as a random shock that generates exogenous variation in a treatment of interest. But surprisingly if effects are constant, the instrumental variables estimator can be consistent even when the relationship between the instrument and the endogenous variable is confounded]]></summary></entry><entry><title type="html">CDD what is it good for?</title><link href="https://macartan.github.io/posts/cdd/" rel="alternate" type="text/html" title="CDD what is it good for?"/><published>2018-07-14T00:00:00+00:00</published><updated>2018-07-14T00:00:00+00:00</updated><id>https://macartan.github.io/posts/cdd</id><content type="html" xml:base="https://macartan.github.io/posts/cdd/"><![CDATA[<ul id="markdown-toc"> <li><a href="#takeaway" id="markdown-toc-takeaway">Takeaway</a></li> <li><a href="#in-detail" id="markdown-toc-in-detail">In detail</a></li> <li><a href="#additional-details-on-point-1" id="markdown-toc-additional-details-on-point-1">Additional details on Point 1</a></li> </ul> <p>There has been a rich discussion going on on blogs and Twitter around evaluations of CDD following the release of a <a href="http://www.3ieimpact.org/media/filer_public/2018/03/12/cdd-brief-wp30.pdf">report</a> by 3ie. See here especially: <a href="https://oxfamblogs.org/fp2p/the-last-word-in-the-community-driven-development-wonkwar-scott-guggenheim-responds-to-howard-white-and-radhika-menon/">https://oxfamblogs.org/fp2p/the-last-word-in-the-community-driven-development-wonkwar-scott-guggenheim-responds-to-howard-white-and-radhika-menon/</a></p> <p>The takeaways are still a bit confusing I think.</p> <h2 id="takeaway">Takeaway</h2> <p>My read is that CDD started off as an approach to use existing social capital to deliver economic benefits more effectively, meaning better choices, perhaps better monitoring, and transfer of the decision-making power to the people that mattered most. That is still what CDD often is. Call it the KDP model. Somewhere along the way an idea was appended that in fact CDD could generate social capital. This version of the CDD model gained currency and influenced the design of many programs, especially in post conflict settings. Call it the CDR model. There are certainly others. The CDR model can differ from the KDP model not just in its goals but in its allocation of funding away from projects and towards activities intended to build social capital. In a way the idea is the antithesis of the original since instead of using existing capital you end up trying to replace it. The idea that local control produces better outcomes likely resonates widely. The idea that short term exposure to CDD alters social capital and local institutions seems a much harder proposition. The evidence on CDDs, I think, largely supports this view. In other words, the KDP model might work well for its purposes; the CDR model seems not to work for its social goals.</p> <p>I think this conclusion has not come out clearly enough in the recent discussion. The discussion has been confused a bit by a denial that CDD has had these social capital ambitions in the first place and perhaps a concern that the KDP model is under attack when the CDR version is challenged.</p> <p>In making sense of this discussion I think five points are worth emphasizing:</p> <ol> <li>Lots of CDD programs have had building social capital as a goal.</li> <li>There is not that much squabbling among the experts and most reviews seem to agree with the propositions that the bottom line that CDD is good for getting stuff to people but not good for changing social structures.</li> <li>This bottom line is important because it matters for fund allocations.</li> <li>This whole conversation is muddied by the diversity of CDD programs and suggests we are using the wrong unit of analysis for studying impact.</li> <li>Although there are now multiple trials of CDR versus nothing, the more fundamental proposition that community driven development performs better than alternatives has still not been as well tested.</li> </ol> <h2 id="in-detail">In detail</h2> <h3 id="point-1">Point 1</h3> <p>Lots of CDD programs <em>have</em> had building social capital as a goal.</p> <ul> <li> <p>3ie claim at the top of their report “CDD programme objectives have evolved over time. The programmes in the early 1990s had more of an emphasis on poverty reduction and infrastructure building; the programmes in the late 1990s and 2000s have focused on decentralisation and improving local governance and social cohesion.”</p> </li> <li> <p>Guggenheim claims “except for one $2.5m project in Sierra Leone that I’ve spent nearly a decade trying to convince my friend Rachel Glennerster was actually a not very well thought through outlier, to the best of my knowledge, no CDD project has had the objective of building new social capital.” But, contrary to Guggenheim’s claim, multiple programs have had new social capital as an explicit goal. (though in some cases, research has focused on social capital related outcomes even though these were not explicit goals of the programs studied)</p> </li> </ul> <p>I give lots of references to back up this claim below.</p> <h3 id="point-2">Point 2.</h3> <p>There is not that much squabbling among the experts and most reviews seem to agree with the propositions that the bottom line that CDD is good for getting stuff to people but not good for changing social structures.</p> <p>Guggenheim writes: “I can tell you that one reason why people resist some of this huge push for evidence-based policy is that there is so much constant squabbling by the specialists about what the evidence actually is.”</p> <p>But in my read this is an area where there is a fairly broad consensus. It is one of few areas in the political economy of development where there has been an accumulation of RCTs and multiple reviews. Of course there are differences between the reviews but I think these agree on a core bottom line. A reflection piece by Sheree Bennett and Alyoscia D’Onofrio <a href="https://www.rescue.org/sites/default/files/document/567/communitydrivenlowresfinalshereeandalyoscia0.pdf">here</a> describes the issues around differences in priorities over “Getting stuff to people or changing how people do things.” The reviews and studies suggest that CDD might be pretty good at the former and not very good at the latter. Wong writes: “nine projects reviewed in this study reported on income poverty impacts as part of their evaluations. Out of the nine, seven had statistically significant positive impacts on household living standards and welfare….Based on the limited evidence to date, most projects have no impact on social capital, or at best mixed impacts. There is very little evidence of social capital spillover effects. For local governance, the evidence is more positive, however also mixed.”</p> <p>Broadly this is also the conclusion in two other very informative reviews:</p> <ul> <li>Elisabeth King and Cyrus Samii in “Fast-Track Institution Building in Conflict-Affected Countries? Insights from Recent Field Experiments”</li> <li>Katherine Casey has a great review of studies here “Radical Decentralization: Does community driven development work?” This is also what I take as an overall conclusion of the 3IE review.</li> </ul> <p>Rachel Glennerster also provides a nice account of discussions on these themes following a World Bank “smackdown” in 2013. Team IE seemed in agreement with the goal of using CDD to “get goods on the ground.”</p> <p>This conclusion resonates with most of what I have seen also.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <h3 id="point-3">Point 3</h3> <p>This bottom line is important because it matters for fund allocations.</p> <p>There are a lot of nuances around what sort of economic benefits arise in what sectors and so on or what sort of social impacts have been assessed where. But still the headline conclusion that CDD may be effective for the hardware but not the software (to use Casey et al’s <a href="https://www.povertyactionlab.org/sites/default/files/publications/45_reshaping%20institutions%20QJE.pdf">terms</a>) is important because a belief that these social benefits derive from CDD can lead to an allocation of funds away from project components that work to those that do not.</p> <h3 id="point-4">Point 4</h3> <p>This whole conversation is muddied by the diversity of CDD programs and suggests we are using the wrong unit of analysis for studying impact. The term CDD gets used quite loosely; some of the interventions studied self-describe as CDR, some as participatory development and so on. Many of the programs are extremely complex with many components. One of the biggest differences is that short term interventions that are not integrated with administrative structures get lumped into longer term institutionalized mechanisms. Inferring failure of the latter from weakness of the first is not helpful. If the packets are so heterogeneous then conclusions might be more clearly stated in terms of the effects of program components — when these can be isolated.</p> <h3 id="point-5">Point 5</h3> <p>Although there are now multiple trials of CDR versus nothing, the more fundamental proposition that community driven development performs better than alternatives has still not been as well tested.</p> <p>The 3ie report says it quite clearly “it is not clear if CDD programmes are a more cost-effective delivery mechanism, especially compared with local government.” Guggenheim writes “to be constructive and useful, the most useful evaluation would have been to compare CDD programs against the next best alternative,” though he did not feel there is a need for an RCT to show it. Glennerster also made the point <a href="http://runningres.com/blog/2013/12/5/smackdown-on-community-driven-development">here</a>. Many of the studies that exist compare a CDD disbursement mechanism coupled with often costly social interventions to a control condition rather than comparing the participatory disbursement mechanisms itself to a viable alternative.</p> <h2 id="additional-details-on-point-1">Additional details on Point 1</h2> <p>I am surprised by Guggenheim’s claim that CDD programs have not aimed at generating social capital. It is possible (and in some ways reasonable) that he excludes CDR programs from his accounting though these are very often considered part of the CDD family. I think there are some instances where researchers have studied the social impacts of CDD programs even if these were not the primary goals of the project.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>In these cases the hypotheses examined appear largely inspired by the broader literature, which is reasonable but perhaps asking a lot of the programs.</p> <p>King and Sami provide the following useful summary of four programs, with variations of social capital appearing regularly (social cohesion, local governance, collective action capacity).</p> <ul> <li> <p><strong>Afghanistan</strong> “The key objective of NSP is to build, strengthen, and maintain Community Development Councils (CDCs) as effective institutions for local governance and social-economic development” (Islamic Republic of Afghanistan, n.d.). Beath et al. add that the program “explicitly mentions promoting gender equality as one of the program’s main goals” (Beath et al., 2012b, p. 6).</p> </li> <li> <p><strong>DRC</strong> “To improve the stability and quality of life for communities in eastern DRC through structured, participatory, and inclusive collective action. By establishing and strengthening participatory local governance committees [the program aims…] to improve the understanding and practice of democratic governance, improve citizens’ relationships with local government, and improve social cohesion and thereby communities’ ability to resolve conflict peacefully. The conduit to achieve these purposes will be village- and community-level projects that themselves will contribute to socio-economic rehabilitation as DRC moves into a post-conflict and development period” (Humphreys et al., 2012, p. 11). “While people’s vision of democracy’s dividends [in anticipation of Congolese elections] is in all probability unrealistic in time and scope, it is nevertheless vital that they receive tangible returns for their enduring tolerance. It is thus crucial that the post-election period deliver peace dividends…” (International Rescue Committee, 2006, p. 11).</p> </li> <li> <p><strong>Liberia</strong> “…the project aims to improve material welfare, build institutions and promote community cohesion by bringing together all actors within the community, including local government, civil society and private sector to identify priority problems/needs and to develop community action plans for implementation.” (International Rescue Committee, 2006b, p. 1).” This model was adopted, in part, as a strategy for using local leadership to quickly generate material improvements in people’s lives. Given the state of the government after fifteen years of civil war communities could plausibly also move more quickly than government to deliver a tangible peace dividend” (Fearon et al., 2008, pp. 2–3).</p> </li> <li> <p><strong>Sierra Leone</strong> “Through intensive, long term facilitation, CDD aims to strengthen local institutions, make them more democratic and inclusive of marginalized groups, and enhance the capacity of communities to engage in collective action”” (Casey et al., 2011c, p. 1).</p> </li> </ul> <p>There are others too.</p> <ul> <li>The BRA-KDP project in <strong>Aceh</strong> (BRA‐KD, US$ 21.7 million, 1,700 conflict‐affected villages) “aimed to deliver quick assistance to conflict‐affected villagers to improve their material wellbeing in the short‐term. In addition, it sought to promote social cohesion, to strengthen village‐level decision‐making institutions, and to cultivate greater faith in governmental institutions in the aftermath of the conflict” (source)</li> <li>Labonne and Chase: <a href="https://www.sciencedirect.com/science/article/pii/S0304387810001008">https://www.sciencedirect.com/science/article/pii/S0304387810001008</a> who in 2010 engage with “One of the claims about CDD approaches is that they enhance community collective action.”) In the program they examine “Community facilitators are the frontline staff working directly with KALAHI-CIDSS communities. They are expected to mobilize their assigned communities, build the latter’s capacity for collective action…” (source)</li> <li>Serena Cocciolo has a series of papers looking at these social outcomes in <strong>Bangladesh</strong></li> </ul> <p>These echo a general statement of the goals of CDR as envisioned by IRC <a href="https://odihpn.org/magazine/community-driven-reconstruction-a-new-strategy-for-recovery/">https://odihpn.org/magazine/community-driven-reconstruction-a-new-strategy-for-recovery/</a></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>A side note on two seeming outliers. A Liberia <a href="https://www.aeaweb.org/articles?id=10.1257/aer.99.2.287">study</a> I worked on seems to contradict this conclusion. For the nuance however the <a href="https://www.cambridge.org/core/journals/american-political-science-review/article/div-classtitlehow-does-development-assistance-affect-collective-action-capacity-results-from-a-field-experiment-in-post-conflict-liberiadiv/142B17BE6422DA333113391802435F81">full study</a> clarifies that the positive effects we find are only inside mixed groups constructed for measurement purposes; we have doubts that these would be the groups in fact tasked to resolve collective action problems in a less controlled setting, of the form studied in <a href="http://www.columbia.edu/~mh2245/papers1/20150519%20HSW.pdf">Congo</a>. The Congo study (which I was also a researcher on) seems to speak against the positive economic conclusions; but a careful read of that study makes clear that it focused specifically on social components and was timed for before major investments were made. For these reasons these studies do not, I think, speak against this broader conclusion. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>For instance Alexandra Adveenko and Michael Gilligan have an interesting paper on “International Interventions to Build Social Capital: Evidence from a Field Experiment in Sudan.” It is not obvious from the paper or the program documentation that this was an aim however: (“The objective of the Project is to meet urgent community-driven recovery and development needs in the war-affected and underdeveloped areas of North Sudan including the Three Areas by providing social and economic services and infrastructure.” <a href="http://documents.worldbank.org/curated/en/799241468334872802/pdf/SU1CDF1PID1Oct030.pdf">here</a> and <a href="http://documents.worldbank.org/curated/en/940461468116354104/pdf/627710PJPR0P0900restructuring0paper.pdf">here</a>. Tu Chi Nguyen and Matthias Rieger have an interesting paper on “Community-driven Development and Social Capital: Evidence from Morocco” where building social capital was not obviously an aim of the program, at least from the description they give <a href="http://cadmus.eui.eu/bitstream/handle/1814/31037/MWP_2014_02.pdf">here</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Contribution to discussion of the benefits of CDD/CDR.]]></summary></entry><entry><title type="html">Gender discrimination in political science and the problem of poor allies</title><link href="https://macartan.github.io/posts/gender-discrimination/" rel="alternate" type="text/html" title="Gender discrimination in political science and the problem of poor allies"/><published>2018-01-07T00:00:00+00:00</published><updated>2018-01-07T00:00:00+00:00</updated><id>https://macartan.github.io/posts/gender-discrimination</id><content type="html" xml:base="https://macartan.github.io/posts/gender-discrimination/"><![CDATA[<p>We have a problem with gender-based discrimination in political science. I know that not because I see it, but because I keep on not seeing it, even as so many of my women students and faculty colleagues are acutely aware of it. <a href="http://www.macartan.nyc/comments/poor-allies/#_ftn1" name="_ftnref1">[1]</a> It works through a multitude of everyday behaviors that add up to unequal treatment and unequal recognition. I describe here, with permission, a number of these as reported to me by students and colleagues in recent weeks. Perhaps most surprisingly, many of the instances of discriminatory behavior described to me involved male friends and colleagues of mine who would likely self-describe as feminists. But they still engage professionally in ways that contribute to everyday discrimination. To be clear, all of these behaviors have been described many times over and better elsewhere. There is nothing new here. But the points bear repeating because the behaviors are pervasive and men are still not seeing them. I start with myself.</p> <p>In early 2016 Alan Jacobs and I presented some of our joint work at Columbia. Besides the two of us there was a chair and two discussants; a third had to drop out. We were all men. And white and middle aged, and in the nomenclature of our profession, senior. The day before the event, a colleague sent me a short note: “when did this become a manel?” she asked. A <a href="http://www.slate.com/blogs/better_life_lab/2017/10/06/there_is_no_excuse_for_all_male_panels_here_s_how_to_fix_them.html">manel</a> is an all-male panel filled with people like me, being all experty on some theme or other. The term has a sting to it; it works its magic just by pointing out an uncomfortable fact and grouping you with any number of other manels you want nothing to do with. You can even get a <a href="http://allmalepanels.tumblr.com/">badge</a>. So I was duly embarrassed.</p> <p>But not quite embarrassed enough to do anything about it. I told myself that although manel problems are real, they don’t <em>always</em> reflect an underlying problem. It could just happen by chance given the small numbers involved. I suggested roping in a female colleague at the last moment to join the panel; this would be a corrective of sorts but one that would leave the woman who volunteered to do the correcting the least well prepared person on the panel.</p> <p>The department chair, Page Fortna, suggested something simpler: that during the session, I just point out the issue, maybe noting that the panel was not representative of the department. She asked me to act as an ally of sorts. I thought about it and decided against it. I decided it would be ungracious to the panel participants, who couldn’t do much about their own gender; that it would have injected uncomfortable politics into an academic discussion; that by drawing attention to the fact, it would make matters worse; and that in any case there was nothing to apologize for since there was no harm meant. In short I came up with an abundance of reasons for inaction.</p> <p>During the same talk I referenced a paper by Jack Snyder and Erica Borghard, but omitting that it was coauthored with Erica.</p> <p>After the talk I learned that a fairly large set of women in the department, both faculty and graduate students, were disappointed by the all male panel, by the fact that I didn’t engage on the issue, and by my selective citing. I was surprised. But even then, I had little difficulty finding excuses for myself. The small numbers argument still worked for me on the gender composition and, embarrassed as I was when the selective citing was pointed out, I explained it to myself on the grounds that I knew Jack better, and of course he was also first author.</p> <p>Looking back I think I found it easy to explain things away because at bottom I thought of myself as someone who does not discriminate and so there must be reasonable explanations for things that others might see as discrimination.</p> <p>With quite a bit of distance though I can see problems with my self explanations. For one, these issues likely would not have arisen in the first place had I the habit of applying the simplest mental checks as guards against unintended discrimination. Had I properly asked myself whether there were women in the department that would have been as or more qualified to participate in the panel as the manel members the answer would have been yes. But I didn’t ask that question, at least not in time. Had I a practice of mentally double checking who the full set of authors are on a paper before citing it, I wouldn’t have left one out. But there is a deeper issue. While I believe that such mental checks can be useful correctives, the deeper problem is that I didn’t feel it necessary to ask myself these questions or respond in any other helpful way, even after women brought concerns directly to my attention. The hubris of my position seems obvious looking back, that I would so easily rely on my own assessment of the effects of my inaction and dismiss the concerns of smart people that were closer to the issue.</p> <p>I think it is fair to say that most of the men that I discussed these events with saw them as small issues; some saw the concerns that the women were raising as somewhat overwrought, as I did at first.</p> <p>In contrast, most women I spoke to did see the problem. And when I discussed the broader issues with my students almost every one of them reported experiencing or observing discrimination, not as unusual events, but as regular, systematic events. Very often this has been through actions of men like me, who think of themselves as concerned about discrimination and tuned in to power inequalities.</p> <p>Here are some of the broader issues they have described to me.</p> <p><strong>Stealing ideas, withholding credit.</strong> In three instances in recent months, women students or junior faculty I have spoken with have felt that their ideas have been stolen from them by more senior men. Research ideas that they shared with male scholars in good faith turn up in new projects and papers, without recognition. In one case a researcher was developing a partnership with an organization for a joint project when a more senior male showed interest and the partners dropped the female researcher. In a fourth instance I heard complaints of a male co-author minimizing a more junior woman’s contributions to joint work. A number of women reported observing that they do not get credited in acknowledgements when they share ideas. In many of these cases the women did not see easy paths to complain and in some cases feared retribution. I was surprised that they would fear retribution, not having seen retribution in our discipline. But that’s the point. They are very conscious of a power dynamic here whereas I am blind to it. Still it’s not hard to see power at play here when you look. I imagine the men involved can convince themselves that the ideas were commonplace, or really their own, or that whatever the origin they were fastest to implement, but I’d bet they’d be more careful in how they treat the ideas if they had originated from a senior male scholar.</p> <p><strong>Seminar cultures.</strong> Many of my students described concerns about seminar cultures. That women’s comments in seminars are less likely to get picked up or responded to; or if they are, only after they are repeated by a man, without acknowledgement. There is even a hashtag for this: #hepetition. This is done by both men and women. But it’s not a hard thing to do a check whether a point has already been made, and if it has been, to build on it with acknowledgement. People seem to do that instinctively already when the comments originate from more senior people.</p> <p>They are also disappointed when they see speaker series filled by men only. They note hostile dynamics in workshops dominated by men who sometimes turn what could be collaborative engagements into competitions, with points going to the players that intellectually pummel their opponents. I was surprised to learn that some women refuse to attend workshops organized by friends of mine because they find the culture toxic. Again, it is not hard to try to make a routine of checking gender balances and asking why they are off. I have often put together a speaker series or a conference and then wondered, sometimes too late, how come it is dominated by men. In my case it sometimes hasn’t taken a lot of puzzling to realize that selections were made on the basis of networks and reputations that reflect exactly the kinds of inequalities that are perpetuated by everyday discrimination.</p> <p><strong>Poor allies.</strong> A last worry raised by women in our discussions was the problem of a lack of allies. They are tired of discussing this issue among women, frustrated by the loss of time and energy spent trying to address it, concerned about being seen as pushy on the issue if they raise it. Perversely there is <a href="https://hbr.org/2016/03/women-and-minorities-are-penalized-for-promoting-diversity">evidence</a> that women are penalized for promoting diversity while men are not. Male colleagues might recognize and respond to extreme instances of discrimination, or try to think through structural responses to gender inequalities, but they shrug off everyday instances of discrimination. Required trainings in implicit discrimination are seen as a chore. Evidence of discrimination is more likely to be dismissed by men (perhaps, as suggested in <a href="https://www.wired.com/story/why-men-dont-believe-the-data-on-gender-bias-in-science/">this</a> piece, because recognizing discrimination implies that your own success might be partly due to biases in the system that favor you rather than being all of your own making). They profess themselves progressive but don’t pay a cost to right inequalities. When it comes down to it they don’t hire the research manager that needs to work inconvenient hours so she can look after children, they don’t give coauthorship to the woman research assistant, they don’t reschedule conferences to avoid weekends even if these are harder for families, they don’t give credit where credit is due.</p> <p>There are many other related issues that were voiced: men get asked about methodological issues in a project while questions about human subjects issues get addressed to women; women are more likely to be chastised for interrupting speakers while men get applauded for mic drop interventions; male students get tapped to give technical support while women are tapped for service work (including one instance where a woman was even asked to organize a date for a visiting male speaker).</p> <p>Some of these worries may seem small on the scale of things. There are big structural issues to worry about also, for sure (see for instance the evidence in this <a href="http://www.saramitchell.org/mitchellgppslides2017.pdf">presentation</a> by Sara Mitchell). And there are horror stories of much more blatantly sexist behavior (see the recent <a href="https://www.bu.edu/polisci/files/2010/10/51.1_Sexual_Harassment_EditedProof1.pdf">APSA report</a> and some of the reports from the <a href="https://twitter.com/ProfessorIsIn/status/946119991467716609">#metooPhD</a> spreadsheet , for instance). But these small everyday things matter a lot because they are about visibility and recognition. Recognition is the currency of our profession. Recognition affects a sense of self worth and the constant denial of it chips away at self confidence. Ultimately, recognition trades in for jobs, salaries, research grants, and influence.<a href="http://www.macartan.nyc/comments/poor-allies/#_ftn2" name="_ftnref2">[2]</a></p> <p>I don’t know what the solutions to all these issues are. I saw last week a psychologist <a href="https://twitter.com/hardsci/status/941471410739806208">nonsense</a> the idea that implicit bias trainings could help, or that such biases can even be noticed or addressed. I hope he is wrong and that at least being aware of the results of biases can tip you off to when you fall prey to them. But whatever the solution it seems a good start to acknowledge the problem and note how it is reproduced even by those of us that like to think the problem lies elsewhere.</p> <p> </p> <p><a href="http://www.macartan.nyc/comments/poor-allies/#_ftnref1" name="_ftn1">[1]</a> Thanks especially to colleagues for frank discussions on these issues which motivated this post: Vanessa Boese, Antje Ellerman, Page Fortna, Alan Jacobs, Sarah Khan, Laura Paler, Gabriella Sacramone-Lutz, Alexandra Scacco, Tara Slough, Anna Wilke, and others.</p> <p><a href="http://www.macartan.nyc/comments/poor-allies/#_ftnref2" name="_ftn2">[2]</a> For quite direct evidence on inequalities of recognition in political science see <a href="https://www.cambridge.org/core/journals/international-organization/article/the-gender-citation-gap-in-international-relations/3A769C5CFA7E24C32641CDB2FD03126A">this</a> study by Maliniak, Powers, and Walter.</p>]]></content><author><name></name></author><category term="professional"/><category term="discussion"/><summary type="html"><![CDATA[We have a problem with gender-based discrimination in political science. I know that not because I see it, but because I keep on not seeing it, even as so many of my women students and faculty colleagues are acutely aware of it. [1] It works through a multitude of everyday behaviors that add up to unequal treatment and unequal recognition. I describe here, with permission, a number of these as reported to me by students and colleagues in recent weeks. Perhaps most surprisingly, many of the instances of discriminatory behavior described to me involved male friends and colleagues of mine who would likely self-describe as feminists. But they still engage professionally in ways that contribute to everyday discrimination. To be clear, all of these behaviors have been described many times over and better elsewhere. There is nothing new here. But the points bear repeating because the behaviors are pervasive and men are still not seeing them. I start with myself.]]></summary></entry></feed>