---
title: "Survey experiments: know your estimand!"
author: Macartan Humphreys
date: 6 December 2025
bibliography: bib.bib
format: 
  html:
    embed-resources: true
    toc: true
    number-sections: true
    linkcolor: darkorange
---

Survey experiments can be used for either **causal inference** (estimating treatment effects) or **descriptive inference** (measuring properties/preferences). But it can sometimes be confusing which of these two worthy goals is the goal in any given experiment. The confusion likely arises in part because very similar designs can be used for either purpose, and in part because survey experimentalists often describe estimands as effects of one kind or another, even if the ultimate inferential target is not an effect. 

The distinction matters because different goals have implications for how you should set things up and  how you should interpret results.  If you are using a survey experiment for descriptive inference, there might be simpler and less noisy strategies available. If you are using it for causal inference, you need to be sure you are clear on what is being manipulated: it mightn't be what you think is being manipulated.

@tbl-survey-experiments summarizes different uses for different types of design, highlighting:

* how the same design might have distinct purposes
* traps to avoid if attracted by this type of survey experiment

| Survey Experiment Type | Causal Inference Use Case | Descriptive Inference Use Case |  Traps |
|:-----------------------|:--------------------------|:-------------------------------|:--------|
| [Priming experiments](#priming-experiments) | Estimate effect of prime on behavior/attitudes (typical) | Use prime as diagnostic to infer knowledge/beliefs (rarer) | Confusing the effect of the prime with the effect of the thing being primed. For example thinking you are finding the effects of exposure to violence by reminding people about past exposure.  |
| [Conjoints](#conjoints) | Estimate effect of feature on choices, given a distribution of other fixed features (rare?) | Make inferences about preferences, classification rules, or ideal points (typical?) | Confusing the (controlled) effects of changing question wording with the (total) effects of intervening on the thing itself. For example thinking you are finding the effects of regime type on willingness to go to war. |
| [List experiments](#list-experiments) | Estimate effect of list length or content on response patterns (rare) | Infer prevalence of sensitive beliefs/behaviors (typical) | Using an experiment for a descriptive quantity might mean accepting too much error in order to  reduce bias.

: Summary of different uses for survey experiments {#tbl-survey-experiments}



The rest of this note just unpacks these ideas, many of which are developed also in @blair2023research (see for example the discussion of the  [Conjoint design](https://book.declaredesign.org/library/experimental-descriptive.html#sec-ch17s3)).

# Concepts

Let's first get clear on the distinction between causal estimands and descriptive estimands and between measurement and inference. (For a bit more, see  [Ch 14](https://book.declaredesign.org/library/) in  @blair2023research.) 

## Inference or measurement?

There's a useful distinction often made between measurement and inference. Measurement is about directly observing a quantity that exists in the world; inference is about estimating a quantity that is at least partly unobserved. So you measure your pulse to make inferences about the state of your heart.

You can have causal inference or descriptive inference, so the measurement / inference distinction is not itself about causality.

Recognizing that you are doing inference rather than measurement in turn helps clarify the need for estimates of uncertainty. If you have data from a sample and you are interested in the sample average, and your quantity is measurable, then just measure; no need for standard errors or similar. If you have sample data and you are interested in the *population* average, and your quantity is measurable, then do inference, and also report your standard errors.

## Causal or descriptive estimands?

A key difference between causal and descriptive estimands is that we generally think that descriptive estimands are, in principle, measurable: they exist, though may be very hard to measure. Causal estimands however involve counterfactual quantities and cannot be measured, even in principle.

This idea builds on a key idea from the  counterfactual model of causation: the causal effect is  the difference between two 'potential outcomes' [[slides](https://macartan.github.io/ci/2.1_causality.html#/causal-claims-the-estimand-and-the-rub)];  that is, between two things that don't in fact exist, things that "could have" happened. When we talk about description however we are usually talking about describing properties that we think things *actually have* , like knowledge, beliefs, values.

Why does this matter? Because it means that if you are interested in causal estimands, then you *have* to do inference. If you are interested in descriptive estimands you may or may not have to do inference. You may be able to measure, but you may have to do inference. That matters  because if your interest is description, maybe you can get away without doing inference. Worth checking. Maybe you can ask everyone in your sample if they like coffee and also if they like tea. You don't have to randomly ask half if they like coffee and half if they like tea and infer the  values for the full sample based on the 'effect' of the question on the answer! Maybe you will find doing it as an experiment rather than a measurement exercise does not add value; or that the possible gains on some fronts, such as reporting biases, don't make up for the cost of having to do inference.

The distinction between descriptive and causal estimands is not always so sharp though. You might question whether all kinds of quantities we might want to describe, preferences, loyalties, and so on *really* exist and can be described in this sense, even in principle. And you might think that some seemingly describable features are themselves causal quantities in disguise. For example you might think of preferences as a summary of the effects of options on choices. So you might think of a quantity such as "being a racist" both as a property that someone has and as a summary of how they react as a function of features of people they encounter. But which way you think about it can have implications for your design.^[We discuss this a bit [here](https://book.declaredesign.org/library/experimental-descriptive.html). As an analogy, we might think of immunity to a disease as a property that someone has, and want to figure out how many have this immunity. We might even be able to measure features that indicate the property (e.g. sickle cell disease for immunity to malaria). In that case we might want to think of this as a descriptive exercise, and even measure the property in a person. But we might also think of immunity as fundamentally about causal relations: that is, we are really asking about how a person would behave in different conditions. A bit more formally one might imagine a world in which $X \rightarrow Y  \leftarrow A$ (all binary nodes) and functional equation $f : Y = XA$. Then $X$ causes $Y$ if and only if $A$ is present. One way of thinking of the problem is to learn about the causal relations captured by $f$, the other is to learn about $A$ --- the value of a node that captures a property that implies a reaction given a background model.]

## Identification

Last, *identification* --- roughly whether you can nail the quantity of interest if you have enough data --- is a problem for inference, but it is not a concern unique to experiments [[slides](https://macartan.github.io/ci/2.2_estimands.html#/identification-1)]. You can have identification problems for causal estimands or descriptive estimands. And of course it bears repeating: even if a quantity is not identified, you can still learn about it!

Let's now use these ideas to think through different uses of survey experiments.

# Survey experiments

The term 'survey experiment' is used to cover a large class of experiments. Some are much like any experiment, aiming to estimate a causal effect of a treatment by manipulating that treatment; others use a manipulation, often of survey wording or procedures, to make it possible to measure -- or at least make inferences about -- a descriptive estimand.  See e.g. Cyrus Samii's [post](https://cyrussamii.com/?p=4168) which makes this distinction, or discussions in @blair2023research.

Sometimes people use the term "survey experiment" specifically for experiments in surveys that use changes in wording or survey protocols to aid descriptive inference. Otherwise just say something clunky like "an experiment embedded in a survey" or "delivered through a survey."

In practice though, it is not obvious  whether an experiment is conducted to aid descriptive inference or causal inference. 

To fix ideas @tbl-ideals describes two ideal cases. 


| **Example of a survey experiment for causal inference** | **Example of a survey experiment for descriptive inference** |
|:---------------------------------------------------------|:------------------------------------------------------------|
| Information experiments are typically used for causal inference, not descriptive inference, whether or not they are delivered through a survey.  In some cases survey-delivered information experiments are almost indistinguishable from field experiments --- for instance if information is delivered in a way similar to treatments of interest and if outcomes are measured outside of the survey, through measures of subsequent behaviors. The key difficulty with embedding an information experiment in a survey is with respect to external validity---whether the effects of information delivered in this way are similar to effects of information delivered in the wild, and so lots of good work in this vein tries to address that head-on. | Randomized response surveys, in which people are randomly assigned to answer either a sensitive question or a non-sensitive question are typically used for descriptive inference [@blair2015design]. The goal is to estimate the prevalence of some property of subjects, such as whether people have engaged in illegal behavior. The randomization makes it possible to make inferences about the prevalence of the sensitive behavior while protecting individual privacy. Here the randomization is a tool to make measurement possible, not the focus of interest itself. There is a causal effect of the procedure on the answer, but the purpose is to make descriptive inferences about something else. | 

: Ideal types, experiment types used clearly for causal inference and for descriptive inference  {#tbl-ideals}


I think these two cases show a  sharp difference between the two goals. The purpose is not always so clear, however. The next sections illustrate  different purposes for common types of survey experiment.


## Priming experiments

Priming experiments can be used for making inferences about both descriptive and causal estimands. 

### A priming experiment conducted for descriptive inference.

Say I am interested in whether you know ($K$) that a weapon was used in a crime. Your knowledge is something I think you have or do not have and I want to know about it. I would love to just measure that evidence, but it is hard.

So I show you a picture ($X$) of the weapon and I measure your reaction ($Y$). I make inferences about the effect of the prime on your reaction ($X$ on $Y$) in order to make inferences about your knowledge ($K$). The effect estimate is a diagnostic tool. I make a causal inference in order to do descriptive inference. But I am clear: my interest is in your knowledge, it is not on the effect of seeing a weapon on your stress levels.

One implication of this is that I would be unhappy with this study if I found no evidence for a causal effect but in fact $K = 1$, or if I did find evidence but $K = 0$; for the simple reason that my interest in the causal effect is just instrumental here.

### A priming experiment conducted for causal inference.

But I might well be interested in a priming experiment specifically to make causal inferences. I am interested in whether being reminded of corruption by a politician makes you more likely to support the opposing party. I am interested in this because I think politicians or the media do this before elections and I am interested in understanding these effects.  If the focus is on the effect of the prime itself this is a standard causal estimand inferred using an experiment, that may or may not just happen to be delivered using a survey.

That makes lots of sense in principle. In practice I think sometimes we see people can trip up and mix up the effect of the prime (e.g. from being reminded that there is corruption) with the effect of the thing being primed (e.g. the effect of corruption itself), or not be clear on whether in fact new information is being provided or not. 


## Conjoints

@de2022improving describe conjoints as "a factorial survey experiment that is designed to measure multidimensional preferences".
Note the emphasis on measurement. Arguably, the remit of conjoints for descriptive inference is a little broader. For example they might also be used to study how people make classifications or understand concepts. But, arguably, conjoints might sometimes also be used when the estimand is causal.

### Conjoints for descriptive inference.

In the many cases in which the goal is to measure preferences, interpretations, or classification rules, conjoint experiments may be best thought of as focused on descriptive inference and using causal inference to make those descriptive inferences.

An example: say a bank uses a rule to decide whether to give loans or not. You want to figure out the rule. You do so using a conjoint to assess which profiles are more likely to get loans given different attributes. The estimand of interest is not a set of causal effects, it is a rule. But you try to figure it out by seeing whether notional features "affect" the classification. 

Two implications from recognizing that the goal here is in fact descriptive inference:

* Opportunity.  You might find out that a more effective strategy would be to figure out the rule from archival sources, such as regulations or instructions to staff. Maybe it is measurable, in which case measure it.

* Risk.  You might fall into the trap of thinking the relation between feature values and outcomes corresponds to the causal effects of changing the feature (or confuse the direct/controlled effect within the experimental regime with the average effect). This is a little trickier, but to think through a simple example: Say in truth we have $X_1 \rightarrow X_2 \rightarrow Y$, and $X_1$ affects $Y$ via $X_2$ but not conditional on $X_2$. Then a conjoint might pick up that $X_1$ is not part of the classification rule for $Y$ and $X_2$ is. But it would be wrong to infer from this that actually changing $X_1$ will not affect classifications (since it might via changes in $X_2$).

For another example, in @hartmann2024trading, we use a conjoint to measure policy preferences. We combine the conjoint results with a choice model to estimate ideal points. Although we use the language of effects a bunch we are interested in trying to measure something but are resorting to using the conjoint to make inferences.

### Conjoints for causal inference

Even still, conjoints can also be used when the primary target is a causal estimand. Say you really are interested in whether the presence of a given feature on a list of features makes it more likely that an outcome will be selected from the list.

You might have an application where people are electing candidates and know nothing about the candidates other than what they get in a flyer. You want to know how features of the flyer affect the choice. Then you are pretty close to the conjoint. You are interested in the effect of the feature on behavior. You have to worry about external validity (is there too much control and all that) but these are common worries for any experiment.

This is the sort of setting discussed in @bansak2023using.

The risk above remains: the effect you are getting is the effect of the attribute on the list, not the average (total) effect of the attribute itself on the outcomes. For example you might find that a powerful candidate does well *given* different values of corruption (even for different distributions of corruption), but this does not give you the effect of power itself, since, after all, power corrupts.


## List experiments

List experiments might also be done for either reason, but the typical use is for descriptive inference.

### A list experiment conducted for descriptive inference.

You are interested in whether people think there is corruption ($K$) or not. In principle this is measurable, but it is hard to measure. You vary whether there is a long list or short list ($X$) and infer from the effects on the count answers ($Y$) whether people think there is corruption or not. You are primarily interested in $K$; there is no independent interest here in how list length affects answer except for its role for descriptive inference.

### A list experiment conducted for causal inference.

I think this is not so common but you could imagine being interested in the effect of a long versus short list on whether people exhibit social desirability bias. Here you are interested in the effect of the length itself, or of the mention of the word itself. @blair2012statistical when describing conditions for valid inference of the descriptive estimand describe a "no design effects" condition that rules out various causal effects. One could in principle be interested in just these (and estimate well if you independently have knowledge of the descriptive estimand)!


There is a good literature comparing experimental and direct approaches for asking sensitive questions. The fact that the estimand is the same in both cases highlights that the focus is typically descriptive. The gain from using an experiment is (hopefully) unbiasedness that comes from providing protection to subjects that require plausible deniability. But the fact that it is an experiment itself implies a cost: you get error from the need to do inference (as well as from complexity; @kuhn2022misreporting) and so [need to determine whether the added error is worth it](https://book.declaredesign.org/library/experimental-descriptive.html#fig-ch17num2).   

# Conclusion

Evergreen advice [@lundberg2021your], but especially important when using a survey experiment: be sure to know your estimand. 
