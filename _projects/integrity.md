---
layout: page
title: Research integrity
img: assets/img/posts/fishy.png
importance: 3
category: methods
---


## Fishing and preregistration

*  Even though everyone knows it is wrong, it is common practice in empirical social science to select what results to report only after analysing data. This practice of “data fishing” can result in enormous bias and an unreliable body of published research. Peter van der Windt and Raul Sanchez de la Sierra and I did a paper on the possible merits of a non-binding but comprehensive registration scheme.  We describe the scope for bias under weak registration systems and discuss likely effects of registration on the sort of research that gets produced and reported [here](http://www.columbia.edu/~mh2245/papers1/PA_2012b.pdf).

* Here is a little [app](https://macartan.shinyapps.io/fish/) I made that illustrated the problem with data fishing.  The point of the app is not to show that generating real random numbers is hard; it is to show that for some problems you can always choose your tests to get the result you want.

*  Al  Fang and Grant Gordon and I have another working paper on the effects of registration in medical sciences. We don’t find much evidence that the way it is done there makes a difference. (Maybe we would have found more if we didn’t preregister) See [here](assets/pdf/papers/2016_FGH_Transparency.pdf).

* See [here](https://docs.google.com/document/d/1hBvLB6LWP9NnpnA4UweMvkKmv15xnnZFv998LU1YedU/edit) for a paper drafted jointly with representatives from three sections at APSA to think through what a APSA sponsored political sciences registry would look like.

* 2013 blog post [Monkey Business](https://www.bitss.org/monkey-business/)

## Related papers


<!-- _pages/publications.md -->
<div class="publications">

  {% bibliography -f papers -q @*[keywords~=integrity]* %}

</div>
